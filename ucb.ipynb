{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upper confidence bound (Multi Armed Bandit) Algorithm\n",
    "\n",
    "(Naveen Mysore, navimn1991@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Multi armed bandit algorithms are a class of powerful algorithms that run the modern world. They are used every where from running clinical trails with RCT, Massive AB Testing to recommeding movies on Netflix. Some empirical studies done by evolutionary biologists seem to show plants and slime molds are good at solving this problem [reference](https://royalsocietypublishing.org/doi/full/10.1098/rsif.2016.0030) which questions the fundamentals of intellgence theory. The problem is still open for an optimal or near optimal solution and studies are still being conducted to find a scalable solution [reference](https://www.nature.com/articles/s41598-021-83726-8). These algorithms are so powerful that, empirical studies are now being conducted in political sciences to see how are they affecting modern democracy [reference](https://www.tandfonline.com/doi/abs/10.1080/08838151.2020.1757365) and polarization in society. If you use any of the internet products like Netflix, TikTok, E commerce platforms or any social media platform then you might have noticed that they recommend products or media contents that so well tuned for you [reference](https://www.youtube.com/watch?v=kY-BCNHd_dM&t=589s&ab_channel=DataCouncil). These systems have modelled your behaviour so well that they do a great job of grabbing your attention. In this article, I will document an empirical study showing how these algorithms work so that we can study their impacts on real world. We will first define the problem and explore solutions like episolon greedy algorithms and upper confidence bound algorithms and lay ground work for contextual bandit algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "[Multi armed bandit algorithms](https://en.wikipedia.org/wiki/Multi-armed_bandit) derive their origins from probablity theory and machine learning. The core of the problem is the exploration - exploitation dilemma that an autonomous agent suffer. An autonomous agent is something that is capable of making its own decision based on received information. For example, You and I autonomous agents since we receive information from our environment and take actions based on some decisions. At every instance of an autonomous agent has to make decisions whether to explore something new or to exploit something which is familiar to it. Based on the decision the agent takes the environment rewards the agent with some positive or negative feedback. The problem is often explained using an analogy of a casino slot machine. You are faced repeatedly with a choice among $k$ different actions or options. After each choice you receive a numerical reward chosen from a stationary probablity distribution that depends on the action you selected. The objective is to maximize the expected total reward over some time period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episolon Greedy and UCB\n",
    "\n",
    "Epsilon-Greedy is a simple method to balance exploration and exploitation by choosing between exploration and exploitation randomly. The epsilon-greedy, where epsilon refers to the probability of choosing to explore, exploits most of the time with a small chance of exploring. Unlike episolon greedy, rather than performing exploration by simply selecting an arbitrary action, chosen with a probability that remains constant, the UCB algorithm changes its exploration-exploitation balance as it gathers more knowledge of the environment. It moves from being primarily focused on exploration, when actions that have been tried the least are preferred, to instead concentrate on exploitation, selecting the action with the highest estimated reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment setup\n",
    "\n",
    "The goal of this experiment is to compare multi arm bandit problem with epsilon $\\epsilon$ greedy approach and upper confidence bound algorithm. In this experiment we will setup $k=10$ arm bandit for the slot machine and compare four epsilon approaches (0.0, 0.01, 0.1, 0.5) followed by upper confidence bound method. We will be running lots of iterations and episodes. Computing averages for these rewards are both space and computationally expensive and hence we will be using running averages. This implies we just need to store two variables in memory ( previous mean and current reward ). Here is the derivation.\n",
    "\n",
    "Let $m_n$ be mean or average value of rewards at $n^{th}$ time step. Let $n$ be the total number of iterations. Let $R_i$ be reward at $i^{th}$ time step.\n",
    "\n",
    "Then\n",
    "\n",
    "\\begin{equation*}\n",
    "m_n = \\frac{\\sum_{i=1}^n R_i}{n}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "m_n = \\frac{(\\sum_{i=1}^{n-1}R_i)+R_n}{n}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "m_n = \\frac{Rn}{n}+\\frac{\\sum_{i=1}^{n-1}R_{n-1}}{n}\n",
    "\\end{equation*}\n",
    "\n",
    "The sum of $n-1$ rewards $\\sum_{i=1}^{n-1}R_{n-1}$ can also be written as $m_{n-1}$*${n-1}$. Using this the previous equation can be written as\n",
    "\n",
    "\\begin{equation*}\n",
    "m_n = \\frac{Rn}{n} + \\frac{(n-1)m_{n-1}}{n}\n",
    "\\end{equation*}\n",
    "\n",
    "Rearranging terms we get\n",
    "\\begin{equation*}\n",
    "m_n = m_{n-1}+\\frac{R_{n}-m_{n-1}}{n}\n",
    "\\end{equation*}\n",
    "\n",
    "Which means, just by keep track of current reward $R_{i}$ and running mean $m_{n-1}$ we can update the new mean $m_{n}$ by above equation.\n",
    "\n",
    "\\begin{equation*}\n",
    "m_n  \\Leftarrow m_{n-1}+\\frac{R_{n}-m_{n-1}}{n}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programatically we can express this as function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_average = lambda m_n_1, r_i, n: m_n_1 + ((r_i - m_n_1) / n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment reward setup\n",
    "\n",
    "We can imagine a slot machine with $k$ arms and for each arm the machine gives us a numerical reward from a sample distribution of mean $mu$ and standard deviation $sd$. We can the model this distrubtion with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardDistribution:\n",
    "    def __init__(self, k=10):\n",
    "        self.k = k\n",
    "        self.mu = 0\n",
    "        self.sigma = 1\n",
    "        self.q_star_mu = np.random.normal(self.mu, self.sigma, k)\n",
    "        self.q_star_sd = np.ones(k)\n",
    "        #print(self.q_star_mu, self.q_star_sd)\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        Rt = np.random.normal(self.q_star_mu[action], self.q_star_sd[action], 1)\n",
    "        return Rt\n",
    "    \n",
    "    def plot(self):\n",
    "        # create a data frame to plot the distribution\n",
    "        df = {}\n",
    "        sample_size = 1000\n",
    "        for action in range(self.k):\n",
    "            mu = self.q_star_mu[action]\n",
    "            sd = self.q_star_sd[action]\n",
    "            df[f'action_{action}'] = np.random.normal(mu, sd, sample_size)\n",
    "        df = pd.DataFrame(data=df)\n",
    "        sns.boxplot(data=df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets model a slot machine with 3 arms and visulize its reward distribution. For each action (pulling a slot arm) the slot machine samples rewards from a distribution of (0,1) and awards it to you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rd = RewardDistribution(k=3)\n",
    "rd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course in reality you would not know this distribution. If you knew it your probabaly would have been a millionaire by now. Even though we wouldn't know this universe encoded distribution we can use the stategic approach of epsilon greedy to modle this distribution.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q values\n",
    "\n",
    "In the literature we model these average estimates as <b>$Q$</b> values. Using a similar defination as above let <b>$R_i$</b> now denote reward received after the <b>$i^ith$</b> iteration for a given action. Let <b>$Q_n$</b> denote the estimate of its action value after it has been selected <b>$n-1$</b> times then\n",
    "\n",
    "\\begin{equation*}\n",
    "Q_n = \\frac{R_1+R_2+R_3+....+R_n-1}{n-1}\n",
    "\\end{equation*}\n",
    "\n",
    "From the derivation made using running averages we can say\n",
    "\n",
    "\\begin{equation*}\n",
    "Q_{n+1} = Q_n + \\frac{1}{n}*(R_n-Q_n)\n",
    "\\end{equation*}\n",
    "\n",
    "This can be informally viewed as this\n",
    "\n",
    "<b>New Estimate</b> $\\leftarrow$ <b>Old Estimate</b> + <b>Step Size</b> (<b>Target</b> - <b>Old Estimate</b>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon bandit\n",
    "\n",
    "In this model we will setup $k=10$ arm bandit for the slot machine and compare four epsilon approaches (0.0, 0.01, 0.1, 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsBandit:\n",
    "    def __init__(self, rd, k, eps, iterations):\n",
    "        self.k = k\n",
    "        self.eps = eps\n",
    "        self.total_avg_reward = 0.0\n",
    "        self.qa = np.zeros(self.k)\n",
    "        self.ac = np.zeros(self.k)\n",
    "        self.iterations = iterations\n",
    "        self.rd = rd\n",
    "\n",
    "    def sample_an_action(self):\n",
    "\n",
    "        def greedy_action():\n",
    "            # pick action corresponding to high qa\n",
    "            return np.argmax(self.qa)\n",
    "\n",
    "        def random_action():\n",
    "            # pick random action from k selections\n",
    "            return np.random.choice(self.k)\n",
    "\n",
    "        if self.eps == 0:\n",
    "            # always greedy choice\n",
    "            return greedy_action()\n",
    "        else:\n",
    "            p = np.random.rand()\n",
    "            # high epsilon means more weight to random actions\n",
    "            if p < self.eps:\n",
    "                return random_action()\n",
    "            else:\n",
    "                return greedy_action()\n",
    "\n",
    "    def execute_an_action(self, action):\n",
    "        sampled_rewards = self.rd.get_reward(action=action)\n",
    "        self.ac[action] += 1\n",
    "        return sampled_rewards\n",
    "\n",
    "    def log(self, t, action, r_t):\n",
    "        print(f'==== step {t} ====')\n",
    "        print(f'Sampled a reward {r_t} for action A_{action}')\n",
    "        print(f'Tr {self.total_avg_reward}')\n",
    "        print(f'qa {self.qa}')\n",
    "        print(f'ac {self.ac}')\n",
    "        print('\\n')\n",
    "\n",
    "    def get_total_average_rewards(self):\n",
    "        return self.total_avg_reward\n",
    "    \n",
    "    def get_action_dist(self):\n",
    "        return self.ac\n",
    "    \n",
    "    def run(self):\n",
    "        avg_reward = [0.0]\n",
    "        for t in range(1, self.iterations):\n",
    "            action = self.sample_an_action()\n",
    "            r_t = self.execute_an_action(action)\n",
    "            self.total_avg_reward = running_average(m_n_1=self.total_avg_reward, r_i=r_t, n=t)\n",
    "            self.qa[action] = running_average(m_n_1=self.qa[action], r_i=r_t, n=self.ac[action])\n",
    "            avg_reward.append(float(self.total_avg_reward))\n",
    "        return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epsilon 0.0 means we always take greedy actions, actions which have maximum rewards in the past. epsilon 0.5 means, 50% of the time we can actions which have maximum rewards in the past and 50% time we will be taking random actions.\n",
    "\n",
    "Ok, now lets run this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(k=5, iterations=1000):\n",
    "    rd = RewardDistribution(k=k)\n",
    "    #rd.plot()\n",
    "    data = {}\n",
    "    \n",
    "    fmt = lambda i, eps, cnt: {'action': f\"action_{i}\", 'count': cnt, 'epsilon': f\"eps_{eps}\"}\n",
    "    \n",
    "    eps_0 = EpsBandit(rd=rd, k=k, eps=0.0, iterations=iterations)\n",
    "    data['eps_0'] = eps_0.run()\n",
    "\n",
    "    eps_0_0_1 = EpsBandit(rd=rd, k=k, eps=0.01, iterations=iterations)\n",
    "    data['eps_0_0_1'] = eps_0_0_1.run()\n",
    "\n",
    "    eps_0_1 = EpsBandit(rd=rd, k=k, eps=0.1, iterations=iterations)\n",
    "    data['eps_0_1'] = eps_0_1.run()\n",
    "    \n",
    "    eps_0_5 = EpsBandit(rd=rd, k=k, eps=0.5, iterations=iterations)\n",
    "    data['eps_0_5'] = eps_0_5.run()\n",
    "\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just ran this experiment for a time step of 1000 and we the collected the running average rewards for different epsilon. As we repeat these experiments our <b>Q</b> values will converge to <b>Q*</b> values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running episodes\n",
    "\n",
    "If you notice we sampled reward distribution one time <b>RewardDistribution</b> and we reused it for different epsilon experiment. We need to repeat this experiment many times with different reward distribution for our values to converge to optimal values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes(episodes=1000):\n",
    "    result = dict()\n",
    "    iterations = 1000\n",
    "    result['eps_0'] = np.zeros(iterations)\n",
    "    result['eps_0_0_1'] = np.zeros(iterations)\n",
    "    result['eps_0_1'] = np.zeros(iterations)\n",
    "    result['eps_0_5'] = np.zeros(iterations)\n",
    "    for episode in range(1, episodes):\n",
    "        df = run_experiment(k=10, iterations=iterations)\n",
    "        result['eps_0'] = running_average(m_n_1=result['eps_0'], r_i=np.asarray(df['eps_0']), n=episode)\n",
    "        result['eps_0_0_1'] = running_average(m_n_1=result['eps_0_0_1'], r_i=np.asarray(df['eps_0_0_1']), n=episode)\n",
    "        result['eps_0_1'] = running_average(m_n_1=result['eps_0_1'], r_i=np.asarray(df['eps_0_1']), n=episode)\n",
    "        result['eps_0_5'] = running_average(m_n_1=result['eps_0_5'], r_i=np.asarray(df['eps_0_5']), n=episode)\n",
    "        _df = pd.DataFrame(result)\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data we need, we shall format it and prepare for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   time_step    epsilon  average_reward\n",
      "0          0      eps_0        0.000000\n",
      "1          0  eps_0_0_1        0.000000\n",
      "2          0    eps_0_1        0.000000\n",
      "3          0    eps_0_5        0.000000\n",
      "4          1      eps_0       -0.066626\n"
     ]
    }
   ],
   "source": [
    "def prepare_data_for_plotting(_df):\n",
    "    entries = []\n",
    "    for time_step in range(0, 1000):\n",
    "        entries.append({'time_step': time_step, 'epsilon': 'eps_0', 'average_reward': _df['eps_0'][time_step]})\n",
    "        entries.append({'time_step': time_step, 'epsilon': 'eps_0_0_1', 'average_reward': _df['eps_0_0_1'][time_step]})\n",
    "        entries.append({'time_step': time_step, 'epsilon': 'eps_0_1', 'average_reward': _df['eps_0_1'][time_step]})\n",
    "        entries.append({'time_step': time_step, 'epsilon': 'eps_0_5', 'average_reward': _df['eps_0_5'][time_step]})\n",
    "    dframe = pd.DataFrame(entries)\n",
    "    return dframe\n",
    "    \n",
    "_df = run_episodes(episodes=100)\n",
    "dframe = prepare_data_for_plotting(_df)\n",
    "print(dframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fcbc8984310>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEHCAYAAABMRSrcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdeXxcZb348c9zZiYzk5nsSZc0Tbe0tOlCKbEtApdd9lpQEARBRCmicq9y9XIFuRT0ily9XlCUHyo7CooLqEXgKmW7FOgOpZTuTZo0+zb7zDnP748zWdombabNZJLm+3695jVnzjlz5jvT5vme8zzPeR6ltUYIIcToZmQ6ACGEEJknyUAIIYQkAyGEEJIMhBBCIMlACCEE4Mx0AEeiuLhYT548OdNhCCHEiLJmzZomrXVJX9tGZDKYPHkyq1evznQYQggxoiildve3TaqJhBBCSDIQQgghyUAIIQSSDIQQQiDJQAghBJIMhBBCIMlACCEEI/Q+AyGEOFZorYlZMSKJiP0wD3g+YHn+mPkcV3jcoMchyUAIIfpgaau7II4mooTNMJFEhKgZJZwIdxfO3a8PU4gfuL33cTQDn1fmX6v+VZKBEGJ0s7RFKB4ilAgRiocIJoKE4iHCiTDBeLB7WzAeJJQIETNjWNrCtEychhNDGcTMGKY2CSVCBGIBAvEAwXhwv8I5akaJmtEjijHLyMLj9OBxevA6vbgdbnvZ4SUnOwePo49tvZY9joO3eRwe3E43XqeXnKycQf5VbZIMhBBpETWjtEXaaIvaj0gigqUtwokwFhZxM95dqHcX7slCvHt9r8I9nAgTToQH/Pluh5ssIwvDMHAoB3ErjqUt3A43CkW2Kxu/y09OVg6l/lK8Dm93Id5VYO+33Ne6A7a7HW4chiONv2r6SDIQQhxEa01nvLO7gO56BOIBOmOd3Y+2aBuNoUbaom1EzWj39kAsQMyKDfjznMpJtivbfjiz8bl8ZDuzyfPndS93r3dl43V6e9a7svdb7truMlxp/IWOPZIMhBgFTMukI9ZBW7SN9mh799l6X8sNoQZqA7XErfghj6lQ5LpzKfGWUOgpJNuVTXlOOb4sHzmuHHKycsj35JPvth9epxelFF6nFwMDl8OFz2kX7i7DhVJqiH4N0RdJBkKMUAkrQVO4idZIK23RNupD9ezu2E17tJ1ALEBdsI7OWCdNkSY6oh39NlI6lIM8d153oX1cwXGcWX4mRZ4i/C7/fmfeOVk53Q+fy4ehpHf6sUKSgRDDQNyMUx+qpyXSst+jq469IdSwX8NmIBZga9vWgxo5ncpJrjuXbGc2E/wTKMorompcVXdB31XoF3gKupf9Lr+clQtJBkIMpZgZY2f7Tqo7q9natpWdbTvZ1bGLHe07+u294lROirOLuxsq3Q43/iw/l824jGn50yhw2wV7sbeYCTkTpK5cHBFJBkIMIq019aF6drTtYEe7/ajurKYp3ERTuIm2aFv3vgpFqb+UybmTqRpXxfT86RR5iyj0FFLoKaTAU4BT2d0hR2oPFTFySDIQIkWWttgX3EdjuJH6YD21gVp2tO9gV8cutrRsIZQIde+bm5XLpNxJlOeUc+LYEynyFlGeU86UvCmU55Tjz/Jn8JsI0SOtyUAp9TBwEdCgtZ7Tx/argH9LvgwAX9Zab0hnTEKkojnczPa27Wxo3EB9qJ49HXt4r+k9AvHAfvsVe4uZmDORpRVLmZY/jSl5U5iaN5VCT6HUx4sRId1XBo8CPwUe72f7TuA0rXWrUup84CFgUZpjEuIg7dF26kP1VHdU8279u2xt3cq2tm20RFq698l35zPON44LplzArKJZjMkew9jssYzNHku+Jz+D0Qtx9NKaDLTWrymlJh9i+//1erkKKEtnPGJ0CyfC1AZqqemsoT5UT32onjX1a9jdsZumcFP3fl6nl+n50zl94ulMy5tGRUEFxxUcR5G3KIPRC5Few6nN4Hrghf42KqVuAG4AKC8vH6qYxAijtaY2WMv2tu1sbd3KltYtBGIBqjur2d2xe7++9oYyqCys5JQJpzAtbxrj/OMY7xtPZWElLof0yBGjy7BIBkqpM7CTwSn97aO1fgi7GomqqqqBD/EnjlmBWIBtbdvY1raNTc2b+Kj1I7a1btuvAXe8bzy5WblMzZvKeVPOY0ruFCbkTGC8b3z3DVVCiGGQDJRS84BfAudrrZszHY8YfoLxIHsDe9nVvouPWj/qfuwN7O3eJycrh5mFM1lasZSKggoq8iuYmjeVPHdeBiMXYuTIaDJQSpUDfwA+p7X+KJOxiOGlNlDLS7te4o3aN1hXv6570DNDGUzOnczc4rl8esanmVEwg2n50xjvGy9DIwhxFNLdtfQ3wOlAsVKqBvgPwAWgtX4QuAMoAn6W7H6X0FpXpTMmMTyF4iHeqn2L1fWrWdewjk3NmwAozynniplXMLtoNpPzJjMtfxpuhzvD0Qpx7El3b6IrD7P9i8AX0xmDGJ5My+TD1g9ZVbuKt+veZl3DOiJmBLfDzZziOSybt4ylFUspy5EOZkIMhYy3GYhjW9yKs6lpE23RNtbWr2VfcB8ftn7IvuC+7olKKvIr+PSMT3Nm+ZnML5kvPXmEyABJBmLQhRNhNjZu5MVdL/Ly7pf3G4+nyFPEzMKZnFx6MnOK57Bo/CKKvcUZjFYIAZIMxCDQWrOjfQcrq1eyYucKtrVtw9IWXqeX08pO49zJ51LsLWZ6wXR8Ll+mwxVC9EGSgTgioXiI12peY33jelZWr+zu5jmveB7L5i3juMLjOLn0ZDxOT4YjFWIE0RpiQQi3QqgJgslHqNfz7Euh4qxB/2hJBiIlnbFO7l97P3/e8WeC8SBZRhYnlZ7E9XOv59QJpzLONy7TIQoxfMSCEG6DcAuEmpMFeotdqHe/bu5ZF24Fs5+5ow0X+IphYnqGb5NkIAakPdrOk5uf5KnNTxGIBbh42sUsrVjKnOI5eJ3eTIcnRPqZ8Z6CO9zS8xxotAvxWACCjfYj0GA/x0P9HEyBtwCyi+xH4RSYsMBe9hbYD18xZBfbz75icOdCGkfAlWQg+mVpi7X1a/nLjr/wws4XCCVCnFV+FjfMu4HKospMhyfE0YuFINhgn6EHGpLLjXYBv9/6Rrvg74/LB1k+8I8BXwlMnJJcLk4W7oU9Bb+vGDz54Bhexe/wikZknKUttrZu5YWdL7Bi5wrqgnV4nV7OmXQO186+lhkFMzIdohD90xoibb0K866CPfn6wOV4sO/juPPsQts/BkpmwORT7OWuAj07Wbh7C+39nCP/RkhJBgKAms4afr7h57xa8yrt0XYcysFJpSfxzwv+mTMmniEDuonMskyItNsFfbjNrqJp2w3t1dC+Fzr2QnsNdNb1XeeujORZefJsvexj9hm8r6TnbL73wzX6Oj5IMhjFLG2xo20Hv9/6e57e8jRO5eTcyedy4tgTObXsVOn/L9IjFkyenXf1lmnsed3Va8aM9Sr82yHa3vexDBfkjofcMpi4EHJLwT+2p9D3j7GXswtB5pE+JEkGo9R7je+x/K3lbGndgqEMllYs5abjb2Ksb2ymQxMjTSLaU6iHDizgmw8u7PtrVHX5ko2mReD0QM54GDPLrl/35tt1772X8yZCzjgp5AeJJINRJBgP8tcdf+W57c+xsXEjY7LHcPui2zml7BQm+CdkOjwxHMXDdjVMZ61dDdOyE1p32usC++zCvr+zdkdWstol2SumeEZPz5juKplePWay5IbETJJkMAp81PoRP133U16pfgWwxwK6+YSbuXLmlfiz/BmOTmRUqKWn3r2zFjrqoKM2WfjvhZYdoM2e/ZUBeWX2Wfn4+T09ZnwlyUK9ZMi6QorBJcngGNYUbuKxTY/x2KbH8Lv8fHbmZzl38rmcMOYElPyRHpsSUejcZxfoZjTZ2Npkn9W377X7w0c77Pr4QMPB3SWVw656yRkPY2bC7KVQND1ZLz/BTgLOrMx8N5FWkgyOQe83vc+DGx7kjb1vYGqT8yefz22Lb5NZv0Y6re3CvGNvsgdNsoAPNtjVNU1boHUXaOvg9xpOu3HVWwiePCgeA+WLoagC8svtBtjcUvtMX+rgRyVJBseIhJXgpV0v8ZsPf8P6xvUUegr5/OzPs2TaEqbmT810eGKgYiG7QG/elnxst58D++wz+QMbXw2nXTXjLYBx82DuZfYZfO4Eu3ukJ99ukJVCXhyGJINjwHuN7/Gt175FTaCG8pxybjz+Rj4/+/MyQuhwpLVdRdNeYzfEtuy0l5u3Qv0HdqHfm3+sffZe9jF7Obc0WV1TZj9LIS8GiSSDEcrSFm/VvsWjmx5lVd0qxmaP5f4z7ue0iafJXMCZFGlPVuHU2tU4HbU9VTpdywfe9ZqVA0VTYdoZdsFfMNl+LpoG7pyMfA0x+kgyGGFiZozntj/Hz9f/nMZwY3d10JfmfYncrNxMh3fssky7vj6YbIwNNdm9cOo/sJ9DzfbwBrHOA96o7AbZ3FIomQkVZ9vLeWV2oV8wxe43Pwi01iQsTTRhYWlNjttJJG7RGooRiCaImxYOQxFPaGKmSTRhgQa3y8DlMCjIzkIpcBgKj9OBBiyt0do+9n6vAcvS9k/Te11/+2t7ue/9NVavfbqeE5bGtCwSpr0cNy0icTvuWPI7mhbJZ/vRe9m0NGbyddzUJEyr+zgaMJRCYXd4choGnuTv0BUDgKHAMBRW8lgJM3ksy8K0emLteo/DsDtmmJbuXo6bPXF17WtZEEmYBCIJTK0xlLK/c684u2LtOpbTUDgdBt84ZwZXLiwflP8zvUkyGEH2dOzhK3//Crs6djG7aDbL5i3j4mkXy1ARR8uMQ7TTLuSbPoKmj9B1G7BadmLEQ8kRKYMo9EFvTeSUEcubQqJwLtaEQuK+UoKecQQ9Ywm6x9LpKiaqHcSSBVg0kSzMGi1idRbRxD5iidrubbGERczs2tfqLvi6t5sW8YRdGFm6pyDsep/uFaLDUJjWwTEfy5QCh1I4jORDKRwOhdMwcDkULoeB01CggK7kAyRMTSRuEjctDENhqJ5Cvasw7iqQXQ4Dp8N+bSiFfTiFUnT/3l2/vVJ20nE67Fi6ju1QCr/bybhcDw7DTgSG6inwXY6uz7Pj7UpECUszsSA9f+9pTQZKqYeBi4AGrfWcPrYr4D7gAiAEfF5rvTadMY1Uj216jPvW3keWI4v7zriPMyaeId1DD0NrTUtrC40124k078YVqCPRWo3RuRd3aB+50XryzSY8OrLf+0wU261StutSQpTQqucQxEuLzqFF59BEHo06j326kEAkGxr7+vQEsDf56J/LochyGGQ5DdxOB1lOeznLYeB22c85HifuXtvtgsIuJLoKG7ez6xj2s0LREoqR63GRn+3C73Z2n/W6HD2foRTdyaYlFANN8urCtM+cFajkGXT36+Qyav+z60Ptr1RPrL337zqG0WsbyQL9wELRYSg8LgOPy4HLYXQX9obRkwDkb+LIpfvK4FHgp8Dj/Ww/H5iefCwCfp58FknvN73Po5se5cVdL3LyhJO5++N3U5JdkumwjlrctOgIxwH7LCqWsKhuDeNzO0iYmiJ/FoFIgqZAjOZglOZAjKZAlM5IgiynQSiWsHtaxhKUWE1Mjm8nv/MjSmI1eMxOiAUZG91FiWqnqNfnmlrRSAFNRjFbXZPpcH+MsCOHmDOHsLuYNt8Uwv5JZPv85HqcRBMWcdPC5TAYbygmGgeevfUUWF1n5T63o7vgdvcq3HsX+O7kOsOQwksMD2lNBlrr15RSkw+xyyeBx7XWGlillMpXSo3XWtelM66RIGEluH/d/Tzy/iNkGVlcP+d6bph3Q9qrhHSvegat7TO4jkgCNDR0RmjojNLQGWFPc5jq1hB17WHArhcdm+uhMNuFqTXxhKYjEqehM4rWdj12WyhOwrK6z1pjiT76w/cfGcWqk3nuWsaa9VQ49jGNvcxjK4X0DIdQTxEdKoe400dd4SJ2F88iq7CcrKJyLH8pJaWTGZPrY5wUwkLsJ9NtBhOA6l6va5LrDkoGSqkbgBsAyssHv/FkOImaUW58+UZW16/m0umXHvEAclpratsj5HtdxBIWptY4kpfRraEYzcEY8YRFjsdFbXuY37yzhzW7WumMJgA7EXicDsJxs8/jF/myKCvw4kxesq/b00pHON591ux1OZhQ4EWhyPEophT7yM5yYGko9GUxId+eIc207Hrb8sJsEqaFw4oQr/uAEloppI3ijs3k1L6Bo22X/cEO7NEqi6bB+POhrMruYz92NmPdfmSoPSFSl+lk0NfpWZ8tXlrrh4CHAKqqqo7ZVrGd7Tu5/c3b2di4kbs+fheXTL+k33211uxoCtISjLG3Ncz66jaagzHe3NaE3+2ksTPab0Hel3G5HpbML6XI74Zkw1pnJMG4PA9OQ1GS42ZMjocxuW7G5nrwu4/yv49l2WPgdN1gtXcP7Hkbat7dfzwcd67dz37hl+whEooq7OESjoEJRYQYLjKdDGqAib1elwG1GYol49Y1rOOm/70JQxl8/9Tvc9HUiw7a5/WtjdS1RdjVHORv7+9jR1NPn3WnYTcknlxRTHMwxgnl+cwpzSMYS5Cd5cDjcnR3hyvwuSjyuXEYivZwnHyviwWTCvC40nQDU6AR6jZA7Tqof9++s7Zl+wF31CoYPw9O/mcoPQHyJ9qDn+WWyo1VQqRZppPB88BXlVJPYzcct4/W9oJ/7PkHt6y8hZLsEh4971FK/aVEEyYvf1DP2ztaqG0LU98Z4f29HYDd6HrS1CKuO2UKk4uyKfK5OW5cTnff5iEVC0H1Ktj1JjRshkTYHt3S6bH75Td9tP+AaAVT7OGMp/yTXdVTPN0+2/ePA0NumBMiE9LdtfQ3wOlAsVKqBvgPwAWgtX4QWIHdrXQbdtfS69IZz3D14zU/5uH3H2Z6wXS+dfx/8/u3g2zcu5p3d7XQFrJ73Mwcl4PH5eDfz5/JWbPGMCbXQ67HNfTBdu6DHSvt6QU7aqF+E1S/DVbCHvGyeLp916y2IBqwhzKuXGIX/uPmwvjj7YHShBDDSrp7E115mO0a+Eo6YxjOtNb8o/ofPPz+w8wvWozVeBlXPripu4vigvJ8bjq9gtOPK8HpyNAZcyIGe9fYCeDDv0L9ez3b3Ln2kMaLb4KJi2DamZAlN8AJMRJluppo1ArFQ3xj5Td4s/ZNco1y3nzrE+S54WtnTueSEyYwuSh7aG6gsSx7wLTWXfbctGbMHl6hYbPdkNu2xz7rR0H5SXDWHVBxjl2f7y1If3xCiCEhySADXq95g39+5WYSVgLazmFfw2lcs6iCfzl7OvnZgzxxSDwCDR9A7dqe+WhDTXYDrhmDQL093MKBcsbDmEqo/CRMOBEmnWxPKi6EOCZJMkgzrTU1rWF+9NIWqlvDVEfeJVzwMJbpIbL3Wqb45/OzmxcwY+xRjk4ZbIbVD9tDIHfuSxb2UXuUTDPas5871x7f3lcChVPsPvoTF0HhVHC4we23h0qWgl+IUUWSQRrEEhYr3qtjze5Wnlld3X2nrSd/A+5xf8JhFTLXuJWLzp3F5VVlqbcHdNTBjlfsap2OWrurZs279hl+V2FfNM0u3KefCxMXQul8u37fcMq8tEKIg0gyGGQf1Xdy45Nr2NFo9/8fn+dhfKFF8aTneKv+VUr9ZfzkzJ9QUVAx8INqbdfjb/s7bH0Ztr1sV/F08Y+FyafC6f8OYysH+RsJIUYDSQaD6KHXtvP9Fz6kMDuLb557HCdXFDOlRPHFl7/IW/Uf8qnpn+K2xbfhMg7TJTTcCvGwfWb/zkPw7i/s8fIBckphwTUw59P23LXZhfZ+0j9fCHEUJBkMgo5InJ+v3M6vXt/JxyYV8rOrF1DsdxMzY9z4vzeypWUL31n8HS4/7vKD32zGYdcbsPv/YN9GaKuGhk377zPjfLvbZlmVfWeuVPMIIQaZJIMjpLVmY007P3xpC29sa0JrWDq/lNsvqqTY7yZuxVn28jJW169m+ceXc+n0S/c/QCIG2/8B/7jbrvMHKD7OntN2wTWQW2bfuDV7KYyZNfRfUAgxqkgyOAKxhMW3//gez66pAWD+xHy+u3QOcybYd9Zqrbll5S2srl/NrQtv7UkElgnrnoTXfwSBBnvYBl8JnH8vHH8leGTaSiFEZkgySNEHtR188oE3iJuazy4q58unTWNiYc9dt1pr7n33Xl6pfoWbT7iZq2ZdZW+o2wArvmkP3VBUAfMuh+POtxt+3f4MfRshhLBJMkiB1prvrfgA09LcdsEsrj9lyn4zVWmt+eZr3+TFXS9y2YzL+MKcL9jdP1++A979pT0G/1n/YY/KKaNwCiGGEUkGKfjfzQ28ua2Zuz85m8+dNPmg7SurV/LirhdZOG4hty++HeO9Z+EPNwAaKs6GJT+xh2MWQohhRpLBAL25rYmvP7OeCflerlx48ExrdYE6vv/O95maN5UHz34QY/Of4Q9fgvxJ8Mmf2sM1CyHEMCXJYABiCYvlf96E22nwi2uqDrpjeFPzJq5ZcQ0azcPnPoxrywvw22vA6YVrnrOHfRBCiGHssMlAKdVJP1NRAmitj/kuMA++up2P6gP84poqKkv3/7oxM8by/1tOnjuPJy94ktL2Ovj99XY30csfl0QghBgRDpsMtNY5AEqpu4B9wBPYcxdfBRzl6GrD39b6Tn7yj61cfHwp51TuP9W61prb37ydzS2b+fFp/03p6sfhzfvBNwa+8DcZ7E0IMWKkMobBuVrrn2mtO7XWHVrrnwOfSldgw4Fpab757Eb8bid3XnzwmD+v1bzGCztf4Kb5N3H2vm2w8vtQvgi+8IIkAiHEiJJKm4GplLoKeBq72uhKwExLVMPEvzyznvXVbdx3xXyK/O6Dtv/ivV9QnlPOF8rPh58utCd9+exvZZwgIcSIk0qp9VngcqA++bgsue6YY1qa8/7nNf68oZazZo5hyfEHdwddU7+GDY0buHzi2bh/d51938AF90oiEEKMSAO6MlBKOYBLtNafTHM8w8K7u1r4cF8nAPddecJB00/GzBg/fOdeipWLy1fcBS4ffOqX9gQxQggxAg3oNFZrbQJHlAiUUucppbYopbYppW7tY3u5UuoVpdQ6pdRGpdQFR/I5g8W0NH/eUAvAn796Cn73Afky3Mqjf/0i77d8wL/vq8Vb9QX48psw88IMRCuEEIMjlTaDN5VSPwWeAYJdK7XWa/t7Q/KK4gHgHKAGeFcp9bzW+oNeu90O/FZr/XOlVCWwApicQlyD6kcvbeGpt/dw9qyxzC3L69mQiMHK/+SNdb/gwZICTg1H+MQJy+Cc5ZkKVQghBk0qyeDjyee7eq3TwJmHeM9CYJvWegeAUupp7CuM3slAA12d9/OA2hRiGlRaa55YtRuAr51Z0XsD/OnL6Pef5Z6yCUxy5fH9Jc9B4bQMRSqEEINrwMlAa33GERx/AlDd63UNsOiAfe4EXlJKfQ3wAWf3dSCl1A3ADQDl5QcPBzEYfvNONZ2RBPdcOpfjJ+b3bNj1Brz/LDXZBex2ObjtxK+RJ4lACHEMSWk4CqXUhcBswNO1Tmt9V//voK8puQ68m/lK4FGt9Y+UUicBTyil5mitrf3epPVDwEMAVVVV/d4RfTT+d3M9xX43l1dN7FlZswae+jTaV8KDCz+F2vUCJ5WelI6PF0KIjBlwP0il1IPAZ4CvYRfylwGTDvO2GqBXyUoZB1cDXQ/8FkBr/RZ2oikeaFyDacu+Tk6pKOoZlnrPKvjlmaAcvHL+nTy/awWXTL+ESbmH+9pCCDGypNIp/uNa62uAVq31cuAk9i/o+/IuMF0pNUUplQVcATx/wD57gLMAlFKzsJNBYwpxDYqEabGvI0JZQXKimkAjPL4UgMjiL/ONtT9kvG88ty++fahDE0KItEslGYSTzyGlVCkQBw45CpvWOgF8FXgR2Izda2iTUuoupdSS5G63AF9SSm0AfgN8XmudlmqgQ9nVHMS0NOVds5atecSelvKyR/mhT2Fqk+vmXIfLcA11aEIIkXaptBn8RSmVD/wXsBa77v8Xh3uT1noFdnfR3uvu6LX8AXByCnGkxS9f34mh4JTpxXbvoY/+BmNmoyuX8sqzP6WyqJLLZ1ye6TCFECItUulNdHdy8fdKqb8AHq11e3rCGnqrdjRz5syxlOZ7Ye9a2LsGzruHO9+6k4ZQA9848Rs4ZKpKIcQxKpUG5NeVUt9TSp0HZB1LiaA9HGdPS4jZXXMVrHkUnB6qp57KH7b+gTJ/GedPOT+jMQohRDql0mZwLbAFe9jq/1NKrVZK/Tg9YQ2tN7Y2YWk47bgSSERh059g9iX8ds+LKBSPnPcIhpIB6IQQx65Uqol2KKXCQCz5OAOYla7AhtKOxgAAleNzYduLEG0nMutinlmznPMmn8c437gMRyiEEOk14GSglNoONAG/Bn4FfO3AG8NGql3NIcbnefC4HLDpj+DJ5w23g3AizCXTL8l0eEKIDNJag9aoAQxPrxMJdDze84jF0NEoViyGjiVfx6LJZ/thRXuWdSIBlolOmGgzAYkEOp4ApcBhgNb4/+k0shecMOjfM5XeRPcDp2DfMXwC8KpS6jWt9fZBj2oIxRIWv19bw/yJ+RDpgA//CnMu5aktzzDeN56qcVWZDlGMIFY4jE4ksAIBUArD7wfLQjmdKJcLnM6DhkTvorVGRyJYwSDasuz+epYJhgPldKDcHqxgAB1PoMMhzEAAIzsbHYlg5ObiLCjACgYxOzsxOzqI19aiHE5A2wVRPNbzHI8fsM5etiJRrHAIEibK5UJlZdm969B2HA4HuJwoZ7LoMC0SDQ2YgQA6YR+HeML+DaIRiCdwFBTgKChAx+NYnR2YgSA6FALDSP4mdscMh88HKLS2wNJgJc81lbKXDQPldIDThXI47BicDlSv1xqNDoXt9VlZGFlZKFcWyu1GuVx2YWsoSJh2oZwwMVtawOXECoawOjvR0Wh3YW7FYuhw2P4NnE77eG63/bs4jF4FvP3ojnkwuVz25yeP7Swsymwy0FrfB9ynlPID12GPKVQGjOguNi9/UA9AQ0cENjwN8SCxBVezYeVNXDXrKrmvYIi3+2wAACAASURBVASwovaZltnaSqK5GR2N4SorwzWhtLvg1VpjBUOYrS2YLS0kWlpAa/uP39JgJkg0NpJoabULAzNhFxiWBQqUMsAw7D960y4odTRKvLbWLgABs7ODRN2+ZOF5CIYBDgeG19tdmFuRCDoSSfdPdRDlcnUX+srlQrndGF4vuJwQj2NFY93fX1uWfaaafADgMHAWl+DIy8PweruTnnI5UVlucDowW9swW1tRWVlkTZ6M4fNjZGcDOlno2seygkG6kg6Gsn9zNFrrnuV4Am2a6ETc/vdJnol3JTC0RmV7IRzHau8gEY/1nJXH43Yy09qOMysLHA4cBfkQjuDIzcVVWorh8ez3HZTXg3I47VijUXQsihWNgmnZv1v3w4WRTDrdiT8ry17XtY8rC+VOJqmuR+/tTqd9BeJ02rE6HP2ePAy2VKqJfoR9ZeAH3gLuAF5PU1xDZmuDPYnNQ9dUwV+WQ+kJrHcaxK04C8YsyHB0xy5tmmCaxKqriW7dSmzXLhINjXbBmjyjdpWW4sjxY3Z0YrbaBbjZ0orZ0mL/wVua2J49xPfs6fMzHMXFKKXQlmUX+rHYYeMy/H6Ux9PzR+lInutYFlpbKCN5BmqaKMPAWToeI9sHWuPJmYmrrAzD58Pw++y3dQbsJJKI2wWpaXUnGisYwIpE7cLH68FwJ5999vFQyi5Uku/R4bAdnysLI9uL4fdjBUModxZWZ6dd4GZn48jNw5Gbg3PMGFAGykgex+VKnim7ul8PVUEjhr9UqolWAfdqrevTFUwmfFDbwdRiH3OKDdj3HvzTN1lVtwqHcvCxcR/LdHhDxkqelRoeT5/btdaYbW0kGhsxm5tJNNojhii3B5XlwllUhHJ7cI0dQ6K5mfjevYQ3bCReW4sVCZOorSNWUwOGwvB4idfVQdfZZRelcOTlYba12WfeyTPu7m35+TgKC3EU5JOorsHw+fBUVpK3ZAlGdjaOwgK7qiQSJdHUSOS99+3qAsOBkePHWViIo6DQ3q+wELTGkZdnn6kbBs7i4n6/vxDHulSSwe+Bzyqlpmit71ZKlQPjtNbvpCm2IbF5XwfzyvJh/W9AW1C2kFUfPcy8knn4s/yZDi8tEi0thDduJFHfgBUOEdm4kc5XVqKjUdwVFTjHjrUvpV0uEk1NKHcWkffeR0ejqX2QUjiKinD4fDiKi/GfcjJmWzuG30/O2WdjeL24yifiOe44siZNQnm99pm8aYJSJBqbsAKddp1zXp592SyESItUksEDgIU9mc3dQCd2ghixp88dkTjVLWGu+Fg5rHscgPaxx7HprU0sm7csw9ENjBmwu8XqaBSzowMj20ds1y6U00Ho3dVEP/oIV+l4e9/OAMFVbxHfvX+1iuH3k3vhBRgeL6G1azBbW8FhYAWCOPLysIIh8j91aXeB7cjNxX3ccaAh0VDf03MiGiW+rx5HXh6u0lI8sytx5OSk/J26Cn3X2DEwdsxR/kJCiIFIJRks0lovUEqtA9BatyZHIh2xPqyz2wvmFWnY9z6c+q+sa9+OpS0WjT9wDp7M06ZJ8K1VBF55xW78MhQtjz8BvatTDmD4fHZjVyKBcrnwzp9P/tKleI8/Hue48TgLCzCys+3jHQH31EOOVSiEGCFSSQbx5JzGGkApVYJ9pTBiba7rAGBO/D1AQ8XZfNiyAYViVmHm76ezolFUVhZmWxvtf/gjbX/4A7Ht+/fkzb3wQtwzZtiNltnZJBobMfw+nEVFZFdV4SwutvtJx+PSYCiE6Feq9xn8ERijlPoe8GnsyexHrIbOCE5DkV//FriyYcKJbNnxWyblTiLblZ2xuCIffEDDffcRfPW1/dZ7Zs9m/Pe+S+5FF2GFQpBI4CwpOezxlFJwhGf+QojRIZX7DJ5SSq3BnohGAUu11pvTFtkQaAnGKfPGUKsfgSmngjOLzS2bmV00OyPxxGtrafjvH9Px4os4fD5yL7wQR0EBzjFj8J20GO/cud37Gm53RmIUQhybBpQMlH3Hx0at9Rzgw/SGNHTaQjH+KWsLhOMw/Vx2te9ib2Avn6v83JB8vtaawKuvEtu+nfjevbT94Y/oSITcJRcz7tvfxpGfPyRxCCHEgJKB1tpSSm1QSpVrrfu+w2cEag3FWOxssF8c/xme3/wEDuXg7PKz0/q5OhZj33e/R+D110nU1XWv91RWMu7uu/DOzsyViRBi9EqlzWA8sEkp9Q4Q7FqptV7S/1uGt9ZgnHLVAN4C8BawvnE9MwtnMtY3Ni2fZ7a3E96wgeZf/orQO+9g5OYy/vvfx3fSYuJ1dXiPP35Ag2EJIcRgSyUZLE9bFBnSGooxIasOCqaQsBK83/Q+l1QM7iil0Z07Cfz975gdnTQ/9FD3+uKvfZXim27q7t3jGifDZAshMieVBuRXD7VdKfWW1vqkow9paGitaQvFKTFqoXAx29q2EU6EmVcyb1COb8Vi1H37Njr+8pf91hfdcAP5n/4UWeXlg/I5QggxGFK5MjicPgd1SU6TeR/26Ka/1Frf08c+l2OPgqqBDVrrzw5iXH0KxUxMM05ebB8UTGZj40aAQUkGZkcHOy68iERjI7kXXoj/9NNxz5iBe3qFVAMJIYalwUwGB43bm7xJ7QHgHKAGeFcp9bzW+oNe+0wH/h04OXlX85CMP9ARiTNeNWNoEzO/nLtX3Q1Amb/sqI7b+cor1H//HhKNjZT8y79QfOPIGNZCCDG6DWYy6MtCYJvWegeAUupp4JPAB732+RLwgNa6FUBr3ZDmmAAIRhNMVPbIm43ZdhfOGQUzjuoOXSsUovab38IKBCj72c/IOfOMQYlVCCHSbTDrLPoqRScA1b1e1yTX9TYDmKGUelMptSpZrZR2gahp9yQCal323blfP/HrR3y8yJaP2PuNW7ACASY99aQkAiHEiJLSlYFSahIwXWv9v0opL+DUWncmN/d1p1ZfCeLA6iQnMB04HXvmtNeVUnO01m0HfPYNwA0A5YPQ+BqIJJiharAcbuowARjvG5/ycaxolJqvfY3ga/Y8P8Vf+yrZJ5541PEJIcRQGvCVgVLqS8CzwP9LrioD/tS1XWv9fh9vqwEm9npdBtT2sc9zWuu41nonsAU7OexHa/2Q1rpKa11VMoDxeA4nGA5xoWMV4eI51IXt+XpSTQaN9/+ELcfP704E+ZddRvFNNx11bEIIMdRSuTL4CnYbwNsAWuutA2jsfReYrpSaAuwFrgAO7Cn0J+BK4FGlVDF2tdGOFOI6Mq27GadaaT7uEuoCdeS58wY8OF2isZGmB/8frU89BUDBVVcx7jsjesw+IcQol0oyiGqtY10NrEopJ330IOpNa51QSn0VeBG7a+nDWutNSqm7gNVa6+eT2z6hlPoAMIFvaq2bj+C7pMToqAHAMX4u+2p/x7jsgd30ZcVi7L7mWmI7d+IoKWbK734nN4wJIUa8VJLBq0qpbwNepdQ5wE3Anw/3Jq31CmDFAevu6LWsgW8kH0PG0bkXAE/RJOq31jPON7ACvf57/0ls505K/vlm8i79lD0blxBCjHCp9Ca6FWgE3gOWYRfwI7ZuxB2sxdIKd+EE9oX2DSgZxGpqaP/Tn/AefzxFN94oiUAIccxIZTgKC/hF8jHiZYfraFQF5GLSHm1nbPbhB6dr+slPQGvGf++7MmOYEOKYMuBkoJRKzg25n3ZgNfDdoajnH0z+yD4aVAmRkH2vweFGKm195re0P/c8hdd/AXdFxVCEKIQQQyaVNoMXsBt4f518fUXyuQN4FLh48MJKv7x4PTWOqQSD9nwCh2tAbv31r3HPmMGYrx/5jWlCCDFcpZIMTtZan9zr9XtKqTe11icrpa4e7MDSSmvy4w10+hbT2GHP1TMxZ2K/u0e2bCG6ZQtjv3M7ypnuETyEEGLopdKA7FdKLep6oZRaCPiTLxODGlW6mTGyiKM9+ezp2IPb4T5kNVH788+D00nu+ecPYZBCCDF0UjnN/SLwsFLKjz3MRAfwRaWUD/h+OoJLm5g9UZvTk8Oezj1MzJmIofrOi9o06fjzX/CfeirOwsKhjFIIIYZMKr2J3gXmKqXyAHXA2EG/HfTI0kjHAijA6fGxp2MDk3In9btv8K1VJBoayLvttqELUAghhliqA9VdCMwGPF1dK7XWd6UhrrSKhTpxA5bLR3VnNaeWndrvvu3PPYeRm4v/jNOHLD4hhBhqqQxU9yDwGeBr2NVElwH9n1IPY/FwAIA2l0nMilGe2/coqFprAq+8Qs4nzsHIyhrKEIUQYkil0oD8ca31NUCr1no5cBL7j0g6YsQj9qjbzUYEgEk5fec0s7UVKxDAM2PGkMUmhBCZkEoyiCSfQ0qpUiAOTBn8kNIvkbwyaDbs5/6uDGK7dgPgKju6qTCFEGK4S6XN4M9KqXzgv4C12Hcjj8ihKcyo3ZuoSXfgdrgZk933GEOBlSvB4cA7f/4QRieEEENvQMlAKWUAf0/2IPq9UuovgEdr3Z7W6NLE7KomMlv67VZqBoK0/fEP+BYvli6lQohj3oCqiZKD1P2o1+voSE0EAFbUrh5qtzr6Ha208+WXMRubKL5x2VCGJoQQGZFKNdFLSqlPAX9IzkEwYlnJm86iVhS/y9/nPuG1a3Dk5eGtqhrK0IQYNeLxODU1NUQikcPvLFLi8XgoKyvD5XIN+D2pJINvAD7AVEqFsbuXaq11bmphDgPRIDHtIGKF8Ll8B222olE6X3oZ7wknyFDVQqRJTU0NOTk5TJ48Wf7OBpHWmubmZmpqapgyZeB9fAbcm0hrnaO1NrTWLq11bvL1yEsEAPEQYdxEEn0ng+jmzZjt7eRdekkGghNidIhEIhQVFUkiGGRKKYqKilK+4krlpjOllLpaKfWd5OuJycHqRp5YgA48RK1In9VE4U2bAPDOnTvUkQkxqkgiSI8j+V1Tuc/gZ9g3mn02+ToAPJDyJw4DRjxIs/ICkO3KPmh7ZNMHOAoLccpE90KIUSKVZLBIa/0Vkjefaa1bgRE5RoMRD1Jv2KHnu/MP2h7ZtAnP7Nly1iKE6Pb8889zzz33AHDnnXfywx/+MMMRDa5UkkFcKeUgOfWlUqoEsA73JqXUeUqpLUqpbUqpWw+x36eVUloplfbuO454kAaHnQwKPAX7bbOiUaLbtuGZXZnuMIQQI8iSJUu49dZ+i7ARL5VkcD/wR2CMUup7wBvAfx7qDcnk8QBwPlAJXKmUOqiUVUrlADcDb6cQzxFzJkI0GXaXqyJP0X7bolu2gGnimT17KEIRQgyRJ598koULFzJ//nyWLVuGaZr4/X5uueUWFixYwFlnnUVjYyMA999/P5WVlcybN48rrrBn+H300Uf56le/etBx169fz+LFi5k3bx6XXHIJra2tAJx++un827/9GwsXLmTGjBm8/vrrQ/dlj0AqvYmeAr6FPZFNHbBUa/27w7xtIbBNa71Dax0DngY+2cd+dwP30jP+UVo5zRBNTvurF3l7koE2Taq/dAMA3kq5MhDiWLF582aeeeYZ3nzzTdavX4/D4eCpp54iGAyyYMEC1q5dy2mnncby5csBuOeee1i3bh0bN27kwQcfPOSxr7nmGn7wgx+wceNG5s6d230MgEQiwTvvvMP//M//7Ld+OBrwfQZKqfuAZ7TWqTQaTwCqe72uARb13kEpdQIwUWv9F6XUvx7i828AbgAoL+97YLmBciWCNHqKcBpOSrwlJJqaaP7lr3COGYPZbt9Y7SwtParPEEIMH3//+99Zs2YNH/vYxwAIh8OMGTMGwzD4zGc+A8DVV1/NpZdeCsC8efO46qqrWLp0KUuXLu33uO3t7bS1tXHaaacBcO2113LZZZd1b+863oknnsiuXbvS8dUGTSrVRGuB25N1//81wLr9vlpgu+9eTo559GPglsMdSGv9kNa6SmtdVVJSMuCg++KywrS4LCb4J+AwHOy7+7u0PPooDffei5GTQ8WrK6XxWIhjiNaaa6+9lvXr17N+/Xq2bNnCnXfeedB+XX/3f/3rX/nKV77CmjVrOPHEE0kkjmyad7fbDYDD4TjiYwyVVKqJHtNaX4Bd9fMR8AOl1NbDvK2G/ec8KANqe73OAeYAK5VSu4DFwPNpbUS2TLKsCCEHFLjtxuPgm292b/YtXoRr7Ni0fbwQYuidddZZPPvsszQ0NADQ0tLC7t27sSyLZ599FoBf//rXnHLKKViWRXV1NWeccQb33nsvbW1tBAKBPo+bl5dHQUFBd3vAE0880X2VMNKkNO1lUgUwE5gMfHCYfd8FpiulpgB7gSvouU+B5GB3xV2vlVIrgX/VWq8+grgGJjkuUcTQ+LJ8JFpasHr9Q4//7nfT9tFCiMyorKzku9/9Lp/4xCewLAuXy8UDDzyAz+dj06ZNnHjiieTl5fHMM89gmiZXX3017e3taK35+te/Tn7+wV3Quzz22GPceOONhEIhpk6dyiOPPDKE32zwqIGOOaeU+gFwKbAdeAb4Y3JI68O97wLgfwAH8LDW+ntKqbuA1Vrr5w/YdyUDSAZVVVV69eojzBcddfDfMzmlbBaLp5/CncZSqr/0JYqWLSN7wQn4R2hWF2Kk2bx5M7NmzcpoDH6/v9+z/pGur99XKbVGa91nzUsqVwY7gY8DUwE3ME8phdb6tUO9SWu9AlhxwLo7+tn39BTiOTIx+x8+Zpj4XX4iG+2hJ4q+eD2OnJy0f7wQQgxHqSQDE/gHdr3/euz6/beAM9MQV/okk0FcJfC5fEQ2bcI1qVwSgRCj0LF6VXAkUulNdDPwMWC31voM4ASgMS1RpVM0gAkkjAR+l5/oRx/hmZnZS1UhhMi0VJJBRGsdAVBKubXWHwLHpSesNIoFCRp29zGfy25Ado7tew5kIYQYLVKpJqpRSuUDfwJeVkq1sn830ZEhFiCYnPPYb3ixOjtxFhQc5k1CCHFsG3Ay0Fp3zfRyp1LqFSAP+FtaokqnWICAYSeDnJDdk8ohyUAIMcodyX0GaK1fHexAhkyvaqLuZJAvyUAIMbql0mZwbIj2XBl4g/bt4XJlIIQYTFprbr75ZioqKpg3bx5r167NdEiHNfqSwb6N7DXyAPAG4gA4CyUZCCEGzwsvvMDWrVvZunUrDz30EF/+8pczHdJhHVE10YjWsoMdRjHQSVZnhDByZSBEpi3/8yY+qO0Y1GNWlubyHxcffl6SJ598kvvvv59YLMaiRYv42c9+Rl5eHsuWLeOVV16hoKCAp59+mpKSEu6//34efPBBnE4nlZWVPP30030e87nnnuOaa65BKcXixYtpa2ujrq6O8ePHD+p3HEyj78rAShBIthk4qvehPB4chxh3RAhx7ErXPAd79+5l4sSeMTrLysrYu3dv2r/P0Rh9VwaWSUjZDceJD7bgnTsX5Rx9P4MQw8lAzuDTIV3zHPQ15ttwHxZ/VF4ZhBQ4lRertRVnSfHh3yOEOCala56DsrIyqqt75vWqqamhdJhPmDXqkoHWJiFDk6W8mB0dGHl5mQ5JCJEh6ZrnYMmSJTz++ONorVm1ahV5eXnDur0ARmk1UUR1JYNmHLmSDIQYrdI1z8EFF1zAihUrqKioIDs7e0TMcTD6koGZIGpo8hIeME0ccmUgxKj2mc98prt9oLe7776bu+++e791b7zxxoCOqZTigQdSmS4+80ZlNVHEsMiPZQHgyMvNcERCCJF5o+/KwEoQMyzyoy4AjFxJBkKI/Q10noNHHnmE++67b791J5988oi7KoBRmQxMooZFXtQBINVEQogjdt1113HddddlOoxBMeqqibBM4oYmJ5a88UySgRBCjL5koLRJQlnkhLtGLJW7j4UQIu3JQCl1nlJqi1Jqm1Lq1j62f0Mp9YFSaqNS6u9KqUnpjMeyEpiGJr8jCg4HzmK56UwIIdKaDJRSDuAB4HygErhSKVV5wG7rgCqt9TzgWeDetAVkWSRrh8htj+IsKUE5HGn7OCGEGCnSfWWwENimtd6htY4BTwOf7L2D1voVrXUo+XIVUJa2aLRJNHlbeV57GNfYsWn7KCHE6JXqfAaPPfYY06dPZ/r06Tz22GOH3Pe2225j4sSJ+P3+wQw57clgAlDd63VNcl1/rgde6GuDUuoGpdRqpdTqxsbGI4vGMokkk4GvPYhzmN8eLoQYmVKZz6ClpYXly5fz9ttv884777B8+XJaW1v73f/iiy/mnXfeGfSY0921tK9h+g4ezg9QSl0NVAGn9bVda/0Q8BBAVVVVn8c4LCthXxlojaelU64MhBguXrgV9r03uMccNxfOv+ewu2V6PoMXX3yRc845h8LCQgDOOecc/va3v3HllVf2eezFixen8CMMXLqvDGqAib1elwG1B+6klDobuA1YorWOpi0abV8Z+CJgRGI4x41L20cJIYa/4TCfwXCZ+yDdVwbvAtOVUlOAvcAVwGd776CUOgH4f8B5WuuGtEZjmYQNRWGn/dI1Tq4MhBgWBnAGnw7DYT6D4TL3QVqvDLTWCeCrwIvAZuC3WutNSqm7lFJLkrv9F+AHfqeUWq+Uej5tAVkJAoZBSbv948uVgRCj23CYz2C4zH2Q9vsMtNYrtNYztNbTtNbfS667Q2v9fHL5bK31WK31/ORjyaGPeBQsk07DYFLy+sM9fXraPkoIMfwNh/kMzj33XF566SVaW1tpbW3lpZde4txzz03PFz6E0TU2kZWg0zAY16YxSopxDHLXLCHEyDIc5jMoLCzkO9/5TndV1R133NHdmNyXb33rW/z6178mFApRVlbGF7/4xT6vZlKl+qqvGu6qqqr06tWrU39j6y5++eipqNdyOC0yiYoX+uzFKoQYAps3b2bWrFmZDqNPfr9/wCOXDld9/b5KqTVa66q+9h9dYxNZJkFD4YmB4c3OdDRCCDFsjKpqokC0k1/m5/Ef8QSOAkkGQoi+pWM+g/fee4/Pfe5z+61zu928/fbbfR570aJFRKP797R/4oknmDt37oBiS9WoSgYfdewAwB0Dle3NcDRCiJEulfkM5s6dy/r16wd87P6SRLqMqmoiZ/KGaE9cqomEEKK3UZUMHGhyQpqyZlDOUXVRJIQQhzSqkoFTKypq7d5TrvKJh9lbCCFGj1GVDEwrTlFyKIqCyy/PbDBCCDGMjLpkUNCpsQBnSUmmwxFCHKNSnc/gvPPOIz8/n4suumiIIjzY6EoGZpzsGESzpM1ACJE+qcxnAPDNb36TJ554Yoii69uoKhETiSgXvjvy7rgW4lj3g3d+wIctHw7qMWcWzuTfFv7bYffL9HwGYI+RtHLlyqP5ukdtVF0ZsG3oxwgXQgxfw2E+g+FiVF0ZmIZcFQgxHA3kDD4dhsN8BsPFqLoyMHXfY48LIUan4TCfwXAxqpKBFY1lOgQhxDAyHOYzGC5GVTWRFZNkIIToMRzmMwA49dRT+fDDDwkEApSVlfGrX/1qyCe4GVXzGaz8+Y2Mve9VXrv6MpbdflcaIhNCDJTMZ5BeMp/BIei4fWUQ9+VkOBIhhBheRlU1kY7ZjT0qy53hSIQQw9lwmM9gqKU9GSilzgPuAxzAL7XW9xyw3Q08DpwINAOf0VrvSkcsOh4HwHLJXAZCiKOXzvkMhlpaq4mUUg7gAeB8oBK4UilVecBu1wOtWusK4MfAD9IVjxWzk4FyyZWBEEL0lu42g4XANq31Dq11DHga+OQB+3wSeCy5/CxwlkrT3RmhkD2FXMwhyUAIIXpLdzKYAFT3el2TXNfnPlrrBNAOFB14IKXUDUqp1Uqp1Y2NjUcUTDiZDKJIMhBCiN7SnQz6OsM/sC/rQPZBa/2Q1rpKa11VcoTDT8c+8zMuWbKcjuzJR/R+IYQ4VqU7GdQAvacUKwNq+9tHKeUE8oCWdATjysoiYvgw+8w/QggxOFKdz8DhcDB//nzmz5/PkiVLhijK/aW7N9G7wHSl1BRgL3AF8NkD9nkeuBZ4C/g08A+dpjvhnIadBBLmyLvRTohj2b7//E+imwd3CGv3rJmM+/a3B/WYA9V7PoO3336bL3/5y4fsQur1ejPe0yitVwbJNoCvAi8Cm4Hfaq03KaXuUkp1pb9fAUVKqW3AN4Bb0xWPy2F/3bglyUAIYXvyySdZuHAh8+fPZ9myZZimid/v55ZbbmHBggWcddZZdLVT3n///VRWVjJv3jyuuOKKfo/Z33wGw1na7zPQWq8AVhyw7o5eyxHgsnTHAeB0dF0ZWEPxcUKIAcrUGXzv+QxcLhc33XTTfvMZ/OhHP+Kuu+5i+fLl/PSnP+Wee+5h586duN1u2tra+j1uf/MZ9DdYXSQSoaqqCqfTya233nrI4bHTZVTdgew0klcGUk0khGD4zGewZ88eSktL2bFjB2eeeSZz585l2rRpR/PVUjaqxiZydV0ZWHJlIIQYPvMZdG2bOnUqp59+OuvWrTuKb3VkRlUycCbbDKQBWQgBw2M+g9bWVqJR+x6opqYm3nzzTSorDxyoIf1GVTWRy5ArAyFEj+Ewn8HmzZtZtmwZhmFgWRa33nprRpLBqJrPIG5afOdP73PT6RWUF2WnITIhxEDJfAbplep8BqPrysBhcM+n5mU6DCGEGHZGVTIQQoiBkPkMhBBiCGmtD9nlcrgbrvMZHEn1/6jqTSSEGD48Hg/Nzc1HVHCJ/mmtaW5uxuPxpPQ+uTIQQmREWVkZNTU1HOmQ9KJ/Ho+HsrKylN4jyUAIkREul4spU6ZkOgyRJNVEQgghJBkIIYSQZCCEEIIRegeyUqoR2H2Eby8GmgYxnJFAvvPoIN95dDia7zxJa93nvMEjMhkcDaXU6v5uxz5WyXceHeQ7jw7/v717j5GrLOM4/v3Z0hbxsq1RU2hNW2wUssqWGGjRP4xCwWpEY4k2RKo0GoIXNBptQ1IkRI1BLWqUQLyQGC7S0giuiQ3WJsb+US6xtIvt2iU10IKWCNZGEgT7+Mf7THsYd+3udHaHmfP7JJOd8553dt7n3IJnNAAABi5JREFUPLv77LnMeyYrZh8mMjMzFwMzM6tnMbi10wPoAMdcD465HiYl5tqdMzAzs/9Vxz0DMzNr4mJgZmb1KgaSLpE0LGlE0tpOj6ddJM2XtE3SHkmPSrom2+dIul/Svvw6O9sl6fu5HXZJOrezEbRG0jRJf5Q0mMsLJe3IeH8haUa2z8zlkVy/oJPjbpWkPkmbJO3NXC+rQY6/mD/TQ5LulDSrF/Ms6aeSDkkaqrRNOLeSVmf/fZJWT2QMtSkGkqYBPwTeB5wNrJI09TcanRwvAl+KiLOApcBnMra1wNaIWAxszWUo22BxPj4N3Dz1Q26La4A9leVvARsy3meBNdm+Bng2It4MbMh+3eh7wG8i4q3AOZTYezbHks4APg+8IyL6gWnAx+jNPN8GXNLUNqHcSpoDXAecD5wHXNcoIOMSEbV4AMuALZXldcC6To9rkmK9F7gIGAbmZttcYDif3wKsqvQ/1q9bHsC8/AV5DzAIiPKpzOnN+Qa2AMvy+fTsp07HMMF4XwPsbx53j+f4DOAJYE7mbRC4uFfzDCwAhlrNLbAKuKXS/pJ+J3rUZs+A4z9YDQeyrafkrvESYAfwxoh4CiC/viG79cK2uAn4CnA0l18H/CMiXszlakzH4s31h7N/N1kEPA38LA+N/VjSafRwjiPiIPBt4HHgKUreHqa381w10dyeVM7rVAxGu7deT11XK+lVwD3AFyLin/+v6yhtXbMtJH0AOBQRD1ebR+ka41jXLaYD5wI3R8QS4F8cP2wwmq6POQ9xXAosBE4HTqMcImnWS3kej7HiPKn461QMDgDzK8vzgCc7NJa2k3QKpRDcHhGbs/lvkubm+rnAoWzv9m3xTuCDkv4C3EU5VHQT0CepccOmakzH4s31rwWemcoBt8EB4EBENO6evolSHHo1xwAXAvsj4umIeAHYDFxAb+e5aqK5Pamc16kYPAgszisRZlBORN3X4TG1hcodxX8C7ImI71ZW3Qc0rihYTTmX0Gi/Iq9KWAocbuyOdoOIWBcR8yJiASWPv4uIy4FtwMrs1hxvYzuszP5d9R9jRPwVeELSW7LpvcCf6NEcp8eBpZJemT/jjZh7Ns9NJprbLcBySbNzr2p5to1Pp0+aTPEJmhXAn4HHgGs7PZ42xvUuyu7gLmBnPlZQjpduBfbl1znZX5Qrqx4DdlOu1uh4HC3G/m5gMJ8vAh4ARoCNwMxsn5XLI7l+UafH3WKsA8BDmedfArN7PcfA9cBeYAj4OTCzF/MM3Ek5L/IC5T/8Na3kFrgy4x8BPjmRMXg6CjMzq9VhIjMzG4OLgZmZuRiYmZmLgZmZ4WJgZma4GJiZGS4GVjM5DfTV+fx0SZum+P0HJK2Yyvc0Gw8XA6ubPuBqgIh4MiJWnqB/uw1QPhBo9rLiD51ZrUi6izL52TDlk51nRUS/pE8AH6LMmd8PfAeYAXwceB5YERHPSDqT8unP1wPPAZ+KiL1jvNdllPnl/0OZQfNCyidDTwUOAt+kTMv8A+BtlMnovhYR9+Z4Pkz5xO1C4I6IuL6tG8OsYvqJu5j1lLVAf0QM5HTfg5V1/ZTpv2dR/mh/NSKWSNoAXEGZDO9W4KqI2CfpfOBHlInyRrMeuDgiDkrqi4h/S1pPmT7gswCSvkGZQ+dKSX3AA5J+m68/L8f0HPCgpF9HxEPt2hBmVS4GZsdti4gjwBFJh4FfZftu4O05RfgFwMYybxpQ/nMfy3bgNkl3U2bcHM1yygysX87lWcCb8vn9EfF3AEmbKXNQuRjYpHAxMDvu+crzo5Xlo5TflVdQbqwyMJ5vFhFX5d7D+4GdkkZ7nYCPRMTwSxrL65qP4fqYrk0an0C2ujkCvLqVF0a5YdD+PBfQuDH5OWP1l3RmROyIiPWUWzDOH+X9twCfyymakbSksu6ivCn6qZTzGdtbGbfZeLgYWK3kYZftkoaAG1v4FpcDayQ9AjxKORk9lhsl7c73+j3wCGUu/rMl7ZT0UeAG4BRgV/a7ofL6P1Cmbd4J3OPzBTaZfDWR2ctQXk107ESz2WTznoGZmXnPwOxkSboWuKypeWNEfL0T4zFrhYuBmZn5MJGZmbkYmJkZLgZmZoaLgZmZAf8FcTytG4ZVzVkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(data=dframe, x='time_step', y='average_reward', hue='epsilon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimistic Initial Values in non stationary environments\n",
    "\n",
    "The methods discussed so far are dependent to some extent on the initial Q values, ie, $Q_1(a)$. In other words these methods are biases with initial values of $Q$. The bias will disappear when all actions are samples infinite time by the law of large numbers. On the other hand in the methods discusses so far we assumed that the enviornment is fixed with a sationary reward distribution. However we know that in real world reward distibution keep changing. So model this problem we need the agent must be encourage to explore more based on the enviorment dynamics. \n",
    "\n",
    "\\begin{equation*}\n",
    "Q_{n+1} = Q_n + \\alpha*(R_n-Q_n)\n",
    "\\end{equation*}\n",
    "\n",
    "Derivation and implemenation details are beyond the scope of this article or may be I will find some time to implmement them in future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upper confidence bound\n",
    "\n",
    "In this model we will use same setup $k=10$ arm bandit for the slot machine and compare four epsilon approaches (0.0, 0.01, 0.1) with upper confidence bound. e-greedy action selection forces the non-greedy actions to be tried, but indiscriminately, with no preferance for those of near greedy. We need to taken into account how close estimates are for other actions too. \n",
    "\n",
    "\\begin{equation*}\n",
    "A_t = argmax(Q_t(a)+c*\\sqrt{\\frac{ln(t)}{N_t(a)}})\n",
    "\\end{equation*}\n",
    "\n",
    "$ln(t)$ denotes the natural logarithm of time steps and $N_t(a)$ represents number of times an action was chosen. In the above equation the square root term measures the uncertainity or variance in the estimates of action values. And $c$ represents the confidence in those estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naveenmysore/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "class UCBandit:\n",
    "    def __init__(self, rd, k, iterations):\n",
    "        self.k = k\n",
    "        self.total_avg_reward = 0.0\n",
    "        self.qa = np.zeros(self.k)\n",
    "        self.ac = np.zeros(self.k)\n",
    "        self.iterations = iterations\n",
    "        self.rd = rd\n",
    "        self.c = 2\n",
    "    \n",
    "    def sample_an_action(self):\n",
    "        try:\n",
    "            uncertainity = np.log(self.iterations)/self.ac\n",
    "        except ZeroDivisionError:\n",
    "            print(\"Initial condition.\")\n",
    "            uncertainity = np.zeros(self.k)\n",
    "        uc = self.c * np.sqrt(uncertainity)\n",
    "        return np.argmax(self.qa+uc)\n",
    "\n",
    "    def execute_an_action(self, action):\n",
    "        sampled_rewards = self.rd.get_reward(action=action)\n",
    "        self.ac[action] += 1\n",
    "        return sampled_rewards\n",
    "\n",
    "    def log(self, t, action, r_t):\n",
    "        print(f'==== step {t} ====')\n",
    "        print(f'Sampled a reward {r_t} for action A_{action}')\n",
    "        print(f'Tr {self.total_avg_reward}')\n",
    "        print(f'qa {self.qa}')\n",
    "        print(f'ac {self.ac}')\n",
    "        print('\\n')\n",
    "\n",
    "    def get_total_average_rewards(self):\n",
    "        return self.total_avg_reward\n",
    "    \n",
    "    def get_action_dist(self):\n",
    "        return self.ac\n",
    "    \n",
    "    def run(self):\n",
    "        avg_reward = [0.0]\n",
    "        for t in range(1, self.iterations):\n",
    "            action = self.sample_an_action()\n",
    "            r_t = self.execute_an_action(action)\n",
    "            self.total_avg_reward = running_average(m_n_1=self.total_avg_reward, r_i=r_t, n=t)\n",
    "            self.qa[action] = running_average(m_n_1=self.qa[action], r_i=r_t, n=self.ac[action])\n",
    "            avg_reward.append(float(self.total_avg_reward))\n",
    "            #self.log(t, action, r_t)\n",
    "        return avg_reward\n",
    "                         \n",
    "def run_experiment(k=5, iterations=1000):\n",
    "    rd = RewardDistribution(k=k)\n",
    "    #rd.plot()\n",
    "    data = {}\n",
    "    \n",
    "    fmt = lambda i, eps, cnt: {'action': f\"action_{i}\", 'count': cnt, 'epsilon': f\"eps_{eps}\"}\n",
    "    \n",
    "    eps_0 = EpsBandit(rd=rd, k=k, eps=0.0, iterations=iterations)\n",
    "    data['eps_0'] = eps_0.run()\n",
    "\n",
    "    eps_0_0_1 = EpsBandit(rd=rd, k=k, eps=0.01, iterations=iterations)\n",
    "    data['eps_0_0_1'] = eps_0_0_1.run()\n",
    "\n",
    "    eps_0_1 = EpsBandit(rd=rd, k=k, eps=0.1, iterations=iterations)\n",
    "    data['eps_0_1'] = eps_0_1.run()\n",
    "    \n",
    "    uc_bandit = UCBandit(rd=rd, k=k, iterations=iterations)\n",
    "    data['uc_bandit'] = uc_bandit.run()\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df = run_experiment()\n",
    "\n",
    "def run_episodes(episodes=1000):\n",
    "    result = dict()\n",
    "    iterations = 1000\n",
    "    result['eps_0'] = np.zeros(iterations)\n",
    "    result['eps_0_0_1'] = np.zeros(iterations)\n",
    "    result['eps_0_1'] = np.zeros(iterations)\n",
    "    result['uc_bandit'] = np.zeros(iterations)\n",
    "    for episode in range(1, episodes):\n",
    "        df = run_experiment(k=10, iterations=iterations)\n",
    "        result['eps_0'] = running_average(m_n_1=result['eps_0'], r_i=np.asarray(df['eps_0']), n=episode)\n",
    "        result['eps_0_0_1'] = running_average(m_n_1=result['eps_0_0_1'], r_i=np.asarray(df['eps_0_0_1']), n=episode)\n",
    "        result['eps_0_1'] = running_average(m_n_1=result['eps_0_1'], r_i=np.asarray(df['eps_0_1']), n=episode)\n",
    "        result['uc_bandit'] = running_average(m_n_1=result['uc_bandit'], r_i=np.asarray(df['uc_bandit']), n=episode)\n",
    "        _df = pd.DataFrame(result)\n",
    "    return _df\n",
    "\n",
    "def prepare_data_for_plotting(_df):\n",
    "    entries = []\n",
    "    for time_step in range(0, 1000):\n",
    "        entries.append({'time_step': time_step, 'algorithm': 'eps_0', 'average_reward': _df['eps_0'][time_step]})\n",
    "        entries.append({'time_step': time_step, 'algorithm': 'eps_0_0_1', 'average_reward': _df['eps_0_0_1'][time_step]})\n",
    "        entries.append({'time_step': time_step, 'algorithm': 'eps_0_1', 'average_reward': _df['eps_0_1'][time_step]})\n",
    "        entries.append({'time_step': time_step, 'algorithm': 'uc_bandit', 'average_reward': _df['uc_bandit'][time_step]})\n",
    "    dframe = pd.DataFrame(entries)\n",
    "    return dframe\n",
    "    \n",
    "_df = run_episodes(episodes=100)\n",
    "dframe = prepare_data_for_plotting(_df)\n",
    "print(dframe.head())\n",
    "\n",
    "sns.lineplot(data=dframe, x='time_step', y='average_reward', hue='algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Associative Search (Contextual Bandits)\n",
    "\n",
    "Associated search involves both trial-and-error learning to search for the best actions and associations of these actions with the situations in which they are best. Associative search tasks are intermediate between the k-armed bandit and the reinforcement learning problem. The scope of study of these algorithms are beyond the scope of this article and I plan to cover them in upcoming articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In these article we defined the problem of explortion/explotiation and its applications in real world. We also demonstated that near greedy solutions are efficient in epsilon greedy settings. We also saw Upper confidence bound algorithms perform better than epsilon greedy algorithms but they are limited to stationary enviornments. We finally laid ground work to associative search or contextual bandits algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
